{
  "schema_name": "tool.code.v1",
  "title": "Ollama Local LLM Tool (Self-Contained)",
  "tags": ["tool", "tool:ollama_local", "workspace:tools", "self-contained", "llm"],
  "context": {
    "name": "ollama_local",
    "description": "Ollama - Fast, free, private local models via Ollama",
    "version": "2.0.0",
    "code": {
      "language": "typescript",
      "source": "/**\n * Ollama LLM Tool - Self-Contained Deno Version\n * Accesses local Ollama models for free, private LLM inference\n */\n\ninterface Message {\n  role: 'system' | 'user' | 'assistant';\n  content: string;\n}\n\ninterface Input {\n  messages: Message[];\n  model?: string;\n  temperature?: number;\n  top_p?: number;\n  ollama_host?: string;\n}\n\ninterface Output {\n  content: string;\n  model: string;\n  usage: {\n    total_tokens: number;\n  };\n  cost_estimate: number;\n}\n\ninterface Context {\n  secrets: Record<string, string>;\n  api: any;\n  request: any;\n}\n\nexport async function execute(input: Input, context: Context): Promise<Output> {\n  // Validate input\n  if (!input.messages || !Array.isArray(input.messages) || input.messages.length === 0) {\n    throw new Error('messages array is required and must not be empty');\n  }\n  \n  const ollamaHost = input.ollama_host || context.secrets['OLLAMA_HOST'] || 'http://localhost:11434';\n  const model = input.model || 'llama3.1:8b';\n  const temperature = input.temperature ?? 0.7;\n  const topP = input.top_p ?? 0.9;\n  \n  // Check Ollama health\n  try {\n    const healthCheck = await fetch(`${ollamaHost}/api/tags`);\n    if (!healthCheck.ok) {\n      throw new Error(`Ollama not available at ${ollamaHost}`);\n    }\n  } catch (error) {\n    throw new Error(`Ollama not available at ${ollamaHost}. Is Ollama running?`);\n  }\n  \n  // Call Ollama API\n  const response = await fetch(`${ollamaHost}/api/chat`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      model,\n      messages: input.messages,\n      options: {\n        temperature,\n        top_p: topP\n      },\n      stream: false\n    })\n  });\n  \n  if (!response.ok) {\n    const errorText = await response.text();\n    throw new Error(`Ollama API error ${response.status}: ${errorText}`);\n  }\n  \n  const data = await response.json();\n  \n  // Ollama returns response in message.content\n  return {\n    content: data.message.content,\n    model: data.model || model,\n    usage: {\n      total_tokens: data.eval_count || 0\n    },\n    cost_estimate: 0 // Always free!\n  };\n}\n"
    },
    "input_schema": {
      "type": "object",
      "properties": {
        "messages": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "role": {
                "type": "string",
                "enum": ["system", "user", "assistant"]
              },
              "content": {
                "type": "string"
              }
            },
            "required": ["role", "content"]
          },
          "description": "Array of chat messages",
          "minItems": 1
        },
        "model": {
          "type": "string",
          "description": "Ollama model to use (e.g., 'llama3.1:8b', 'codellama:7b')",
          "default": "llama3.1:8b"
        },
        "temperature": {
          "type": "number",
          "description": "Sampling temperature (0-2)",
          "minimum": 0,
          "maximum": 2,
          "default": 0.7
        },
        "top_p": {
          "type": "number",
          "description": "Nucleus sampling parameter",
          "minimum": 0,
          "maximum": 1,
          "default": 0.9
        },
        "ollama_host": {
          "type": "string",
          "description": "Ollama host URL",
          "default": "http://localhost:11434"
        }
      },
      "required": ["messages"]
    },
    "output_schema": {
      "type": "object",
      "properties": {
        "content": {
          "type": "string",
          "description": "The AI-generated response"
        },
        "model": {
          "type": "string",
          "description": "Model that generated the response"
        },
        "usage": {
          "type": "object",
          "properties": {
            "total_tokens": { "type": "number" }
          },
          "description": "Token usage statistics"
        },
        "cost_estimate": {
          "type": "number",
          "description": "Always 0 for local models"
        }
      },
      "required": ["content", "model", "usage"]
    },
    "permissions": {
      "net": ["localhost:11434"],
      "read": false,
      "write": false,
      "env": false,
      "run": false,
      "ffi": false,
      "hrtime": false
    },
    "limits": {
      "timeout_ms": 120000,
      "memory_mb": 256,
      "cpu_percent": 80
    },
    "required_secrets": [],
    "ui_schema": {
      "configurable": true,
      "config_fields": [
        {
          "key": "ollamaHost",
          "label": "Ollama Host",
          "type": "string",
          "ui_type": "text",
          "description": "Ollama server endpoint",
          "default_value": "http://localhost:11434",
          "required": false,
          "placeholder": "http://localhost:11434",
          "help_text": "URL where Ollama is running"
        },
        {
          "key": "defaultModel",
          "label": "Default Model",
          "type": "string",
          "ui_type": "text",
          "description": "Default Ollama model to use",
          "default_value": "llama3.1:8b",
          "required": false,
          "placeholder": "llama3.1:8b",
          "help_text": "Model name (e.g., llama3.1:8b, codellama:7b, mistral)"
        },
        {
          "key": "temperature",
          "label": "Temperature",
          "type": "number",
          "ui_type": "slider",
          "description": "Response creativity (0.0 - 2.0)",
          "default_value": 0.7,
          "required": false,
          "validation": {
            "min": 0.0,
            "max": 2.0,
            "step": 0.1
          }
        },
        {
          "key": "topP",
          "label": "Top P",
          "type": "number",
          "ui_type": "slider",
          "description": "Nucleus sampling parameter (0.0 - 1.0)",
          "default_value": 0.9,
          "required": false,
          "validation": {
            "min": 0.0,
            "max": 1.0,
            "step": 0.05
          }
        }
      ]
    },
    "examples": [
      {
        "description": "Simple local query",
        "input": {
          "messages": [
            { "role": "user", "content": "What is the capital of France?" }
          ]
        },
        "output": {
          "content": "The capital of France is Paris.",
          "model": "llama3.1:8b",
          "usage": { "total_tokens": 25 },
          "cost_estimate": 0
        },
        "explanation": "Free local inference. Access response with result.content."
      },
      {
        "description": "Code generation",
        "input": {
          "messages": [
            { "role": "user", "content": "Write a Python function to reverse a string" }
          ],
          "model": "codellama:7b"
        },
        "output": {
          "content": "def reverse_string(s):\\n    return s[::-1]",
          "model": "codellama:7b",
          "usage": { "total_tokens": 40 },
          "cost_estimate": 0
        },
        "explanation": "Use specialized models like codellama for coding tasks. Always free!"
      }
    ]
  }
}

