# RCRT Vector Search: Complete Implementation

## ğŸ” **The Full Flow**

```
User Query: "What was that API key?"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. HTTP Request                                             â”‚
â”‚    GET /breadcrumbs/search?q=What+was+that+API+key&nn=5    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Query params
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Endpoint Handler (main.rs:176-248)                      â”‚
â”‚    async fn vector_search(Query(q): Query<SearchQuery>)    â”‚
â”‚                                                             â”‚
â”‚    Parameters:                                              â”‚
â”‚    â€¢ q: Option<String>           "What was that API key?"  â”‚
â”‚    â€¢ qvec: Option<String>        None (will embed q)       â”‚
â”‚    â€¢ nn: Option<i64>             5 (neighbors)             â”‚
â”‚    â€¢ tag: Option<String>         None                      â”‚
â”‚    â€¢ include_context: bool       false (default)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Process query
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Text Embedding (main.rs:266-328)                        â”‚
â”‚    fn embed_text(text: String) -> Result<Vec<f32>>         â”‚
â”‚                                                             â”‚
â”‚    Step 1: Load ONNX model (lazy, cached)                  â”‚
â”‚      TOKENIZER: models/tokenizer.json (MiniLM)            â”‚
â”‚      SESSION: models/model.onnx (384d output)             â”‚
â”‚                                                             â”‚
â”‚    Step 2: Tokenize input                                  â”‚
â”‚      "What was that API key?"                              â”‚
â”‚      â†’ [101, 2054, 2001, 2008, 3927, 3145, 102]          â”‚
â”‚        (BERT WordPiece tokens)                             â”‚
â”‚                                                             â”‚
â”‚    Step 3: Run ONNX inference                              â”‚
â”‚      Inputs:                                                â”‚
â”‚        input_ids: [101, 2054, 2001, ...]                  â”‚
â”‚        attention_mask: [1, 1, 1, ...]                     â”‚
â”‚        token_type_ids: [0, 0, 0, ...]                     â”‚
â”‚                                                             â”‚
â”‚      Output: Tensor<f32> [1, 384]                          â”‚
â”‚        [0.012, -0.043, 0.091, ..., 0.023]                 â”‚
â”‚                                                             â”‚
â”‚    Step 4: Mean pooling (if output > 384)                  â”‚
â”‚      Average token embeddings â†’ sentence embedding         â”‚
â”‚                                                             â”‚
â”‚    Step 5: L2 Normalization                                â”‚
â”‚      norm = sqrt(sum(xÂ²)) = 1.0                            â”‚
â”‚      vec = vec / norm                                       â”‚
â”‚      â†’ Unit vector for cosine similarity                   â”‚
â”‚                                                             â”‚
â”‚    Result: Vec<f32> (384 dimensions, L2-normalized)        â”‚
â”‚      [0.031, -0.112, 0.237, ..., 0.059]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Query vector ready
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Build SQL Query (main.rs:194-197 or 224-227)           â”‚
â”‚                                                             â”‚
â”‚    let sql = "                                              â”‚
â”‚      SELECT id, title, context, tags, schema_name,         â”‚
â”‚             version, updated_at                             â”‚
â”‚      FROM breadcrumbs                                       â”‚
â”‚      WHERE owner_id = $1                                    â”‚
â”‚      ORDER BY embedding <#> $2                             â”‚
â”‚      LIMIT $3                                               â”‚
â”‚    ";                                                       â”‚
â”‚                                                             â”‚
â”‚    Operator: <#>                                            â”‚
â”‚      = Cosine distance (pgvector operator)                 â”‚
â”‚      = 1 - cosine_similarity(a, b)                         â”‚
â”‚      = Lower is better (0 = identical)                     â”‚
â”‚                                                             â”‚
â”‚    Bind parameters:                                         â”‚
â”‚      $1: owner_id (tenant isolation via RLS)               â”‚
â”‚      $2: qvec (query embedding vector)                     â”‚
â”‚      $3: limit (nn parameter)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Execute SQL
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. PostgreSQL + pgvector Extension                         â”‚
â”‚                                                             â”‚
â”‚    Index: ivfflat (migration 0001_init.sql:56)             â”‚
â”‚      CREATE INDEX idx_breadcrumbs_embedding                â”‚
â”‚      ON breadcrumbs                                         â”‚
â”‚      USING ivfflat (embedding vector_cosine_ops);          â”‚
â”‚                                                             â”‚
â”‚    Query execution:                                         â”‚
â”‚      1. Filter: owner_id = $1 (RLS isolation)             â”‚
â”‚      2. Index scan: ivfflat approximate nearest neighbor   â”‚
â”‚      3. Distance calc: cosine_distance(embedding, $2)      â”‚
â”‚      4. Sort by distance (ascending = closest first)       â”‚
â”‚      5. Limit to nn results                                â”‚
â”‚                                                             â”‚
â”‚    Example results (with distances):                       â”‚
â”‚      Row 1: "I mentioned OPENROUTER_API_KEY..." (0.08)     â”‚
â”‚      Row 2: "Set your API key in secrets" (0.11)           â”‚
â”‚      Row 3: "The key format is sk-or-..." (0.13)           â”‚
â”‚      Row 4: "You can find your key at..." (0.15)           â”‚
â”‚      Row 5: "Store keys in .env" (0.17)                    â”‚
â”‚                                                             â”‚
â”‚    Note: Distances NOT returned to client (just ordering)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Results ready
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Return to Client (main.rs:216-221)                      â”‚
â”‚    Json<SearchResult::Context>                              â”‚
â”‚                                                             â”‚
â”‚    [                                                        â”‚
â”‚      {                                                      â”‚
â”‚        id: "abc...",                                        â”‚
â”‚        title: "Extension Chat Message",                    â”‚
â”‚        context: { content: "I mentioned OPENROUTER..." },  â”‚
â”‚        tags: ["user:message"],                             â”‚
â”‚        schema_name: "user.message.v1",                     â”‚
â”‚        version: 1,                                          â”‚
â”‚        updated_at: "2025-10-03T..."                        â”‚
â”‚      },                                                     â”‚
â”‚      ... (4 more results)                                  â”‚
â”‚    ]                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¢ **The pgvector Operators**

RCRT uses **cosine distance** (`<#>`) - here's why:

```sql
-- Available pgvector operators:
<->   L2 distance (Euclidean)
<#>   Cosine distance (1 - cosine similarity)  â† RCRT uses this
<=>   Inner product distance

-- Why cosine?
-- L2-normalized embeddings + cosine distance = optimal for semantic similarity
```

**Mathematical detail:**
```
cosine_similarity(a, b) = (a Â· b) / (||a|| Ã— ||b||)

Since RCRT L2-normalizes embeddings (main.rs:326-327):
  ||a|| = 1, ||b|| = 1

Therefore:
  cosine_similarity(a, b) = a Â· b
  cosine_distance(a, b) = 1 - (a Â· b)

Range: [0, 2] where:
  0.0 = identical vectors (perfect match)
  1.0 = orthogonal (no similarity)
  2.0 = opposite directions (anti-similar)
```

## ğŸ§  **ONNX Embedding Model**

**Model:** `sentence-transformers/all-MiniLM-L6-v2`

**Specs:**
- **Dimensions:** 384
- **Input:** Text (up to 512 tokens)
- **Output:** Dense vector [f32; 384]
- **Speed:** ~50ms per embedding (CPU)
- **Quality:** 0.68 on STS benchmark

**How it works:**

```rust
// 1. Tokenization (main.rs:277)
let encoding = tokenizer.encode(text, true)?;
let ids = encoding.get_ids();
// "What was that API key?"
// â†’ [101, 2054, 2001, 2008, 3927, 3145, 102]
//    [CLS] What  was  that  API   key   [SEP]

// 2. Create BERT inputs (main.rs:279-282)
input_ids:      [101, 2054, 2001, 2008, 3927, 3145, 102]
attention_mask: [  1,    1,    1,    1,    1,    1,   1]
token_type_ids: [  0,    0,    0,    0,    0,    0,   0]

// 3. ONNX forward pass (main.rs:288-299)
let outputs = session.run(inputs)?;
// Shape: [1, 7, 384] (batch=1, tokens=7, hidden=384)

// 4. Mean pooling (main.rs:304-315)
for each token embedding:
    acc[i] += token_embedding[i]
acc = acc / num_tokens
// Result: [1, 384]

// 5. L2 normalization (main.rs:326-327)
norm = sqrt(sum(vec[i]Â²))
vec = vec / norm
// Result: Unit vector with ||vec|| = 1.0
```

## ğŸ“Š **Index Strategy: ivfflat**

**What is ivfflat?**
- **IVF:** Inverted File index (clusters vectors into buckets)
- **Flat:** Flat vectors within each bucket
- **Tradeoff:** Speed vs accuracy

```sql
-- Index definition (migrations/0001_init.sql:56)
CREATE INDEX idx_breadcrumbs_embedding 
ON breadcrumbs 
USING ivfflat (embedding vector_cosine_ops);
```

**How it works:**

```
Database has 1000 breadcrumbs with embeddings:
  â†“
ivfflat creates ~100 clusters (automatic)
  Cluster 1: Breadcrumbs about "API keys" [10 vectors]
  Cluster 2: Breadcrumbs about "docker" [12 vectors]
  Cluster 3: Breadcrumbs about "agents" [15 vectors]
  ...
  Cluster 100: Breadcrumbs about "errors" [8 vectors]
  â†“
Query: "What was that API key?" (embedded)
  â†“
1. Find closest cluster centroid (fast)
   â†’ Cluster 1: "API keys" (distance: 0.12)
  â†“
2. Search within that cluster (exact)
   â†’ Top 5 results from Cluster 1
  â†“
Return results (very fast for 1M+ vectors)
```

**Performance:**
```
Without index (full scan):
  O(N) where N = total breadcrumbs
  1000 breadcrumbs = 1000 distance calculations
  
With ivfflat:
  O(K + L) where K = clusters, L = cluster size
  1000 breadcrumbs = 100 clusters + 10 per cluster
  ~100x faster!
```

## ğŸ¯ **SDK API Usage**

### **Option 1: Text Query (Automatic Embedding)**

```typescript
const results = await client.vectorSearch({
  q: "What was that API key?",  // Server embeds this
  nn: 5,
  filters: { tag: 'workspace:agents' }
});
```

**Flow:**
```
Client sends: q=text
  â†“
Server: embed_text(q) â†’ Vec<f32>
  â†“
PostgreSQL: ORDER BY embedding <#> vec
  â†“
Returns: Top 5 results
```

### **Option 2: Pre-Computed Vector**

```typescript
// If you already have an embedding
const queryVec = await embedText("What was that API key?");

const results = await client.vectorSearch({
  qvec: queryVec,  // Skip server-side embedding
  nn: 5
});
```

**Use case:** Batch queries with same embedding

### **Option 3: Combined with Filters**

```typescript
const results = await client.vectorSearch({
  q: "API key",
  nn: 10,
  filters: {
    tag: 'workspace:agents',
    schema_name: 'user.message.v1',
    // Only search in user messages
  }
});
```

**SQL Generated:**
```sql
SELECT ...
FROM breadcrumbs
WHERE owner_id = $1
  AND $3 = ANY(tags)              -- Tag filter
  AND schema_name = 'user.message.v1'  -- Schema filter (NOT IN CURRENT CODE!)
ORDER BY embedding <#> $2
LIMIT 10;
```

## âœ… **Now Supports: schema_name Filter** (Fixed!)

```rust
// main.rs:194-204 (updated)
let mut sql = String::from("... WHERE owner_id = $1");
let mut bind_idx = 3;

if q.tag.is_some() { 
    sql.push_str(&format!(" and ${} = any(tags)", bind_idx)); 
    bind_idx += 1;
}

if q.schema_name.is_some() { 
    sql.push_str(&format!(" and schema_name = ${}", bind_idx)); 
}

sql.push_str(" order by embedding <#> $2 limit ...");
```

**Now works:**
```typescript
await vectorSearch({
  q: "API key",
  filters: { schema_name: 'user.message.v1' }  // âœ… Filters to only user messages!
});
```

See [VECTOR_SEARCH_IMPLEMENTATION.md](./VECTOR_SEARCH_IMPLEMENTATION.md) for performance details and optimization strategies.
